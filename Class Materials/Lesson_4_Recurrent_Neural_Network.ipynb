{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6HlcB95tplJ"
   },
   "source": [
    "# **Hands-on RNN with TensorFlow**\n",
    "This notebook will guide you through building and training a basic Recurrent Neural Network (RNN) model using TensorFlow. We'll explore the concepts learned in the lecture and put them into practice by performing sentiment analysis on a Natural Language Processing (NLP) dataset.\n",
    "\n",
    "## **Prerequisites:**\n",
    "\n",
    "* Basic understanding of Python programming\n",
    "* Familiarity with Machine Learning concepts\n",
    "* Introduction to Deep Learning (activation functions, optimizers)\n",
    "\n",
    "## **Learning Objectives:**\n",
    "\n",
    "* Implement a basic RNN model in TensorFlow\n",
    "* Train the model on an NLP dataset\n",
    "* Evaluate the model's performance\n",
    "* Make predictions using the trained model\n",
    "* Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4so0fZ3TtPCC"
   },
   "source": [
    "## Loading and Preprocessing the IMDB Review Dataset\n",
    "\n",
    "In this step, we'll load and preprocess the IMDB movie review dataset for our RNN sentiment analysis task. Sentiment analysis aims to automatically determine the overall opinion or feeling expressed in a piece of text. Here, we're specifically trying to classify reviews as either positive (expressing a good opinion about the movie) or negative (expressing a bad opinion).\n",
    "\n",
    "The `imdb.load_data` function provides a convenient way to access the IMDB dataset, which includes reviews labeled as positive or negative. We'll use this data to train our RNN model to identify sentiment patterns in text.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Data Loading:** We load a subset of the IMDB dataset using `imdb.load_data()`, specifying the maximum number of words to consider (10,000 in this case) to reduce vocabulary size and training complexity.\n",
    "\n",
    "2. **Data Splitting:** The loaded data is split into training and testing sets. The training set will be used to train the model, and the testing set will be used to evaluate its performance on unseen reviews.\n",
    "\n",
    "3. **Text Preprocessing:**\n",
    "    - **Text to Sequences:** Reviews, which are initially strings of text, are converted into sequences of integer indices using `text_to_word_sequence`. This allows the model to process the text data numerically.\n",
    "    - **Word Indexing:** Each word in the vocabulary is mapped to a unique integer index using a dictionary (`word_index`). This dictionary is retrieved from the tokenizer used during data loading by `imdb.get_word_index()`.\n",
    "    - **Padding:** Sequences can have varying lengths. We use `pad_sequences` to ensure all sequences have a uniform length (100 in this case) by padding shorter sequences with zeros at the end or truncating longer ones. Padding is essential for feeding sequences into RNN models.\n",
    "\n",
    "By following these steps, we prepare the IMDB review data for training our RNN model to perform sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyCZa6GJt_uI"
   },
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uu1n7hYQtoAQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlUDHbGjt3E4"
   },
   "source": [
    "We'll import necessary libraries:\n",
    "\n",
    "* `tensorflow`: The core library for building and training deep learning models.\n",
    "* `keras`: A high-level API built on top of TensorFlow for easier model development.\n",
    "* `imdb dataset loader`: We'll use a subset of the IMDB movie review dataset for this exercise.\n",
    "* `pad_sequences`: Imports a function for padding text sequences to a uniform length (important for RNNs).\n",
    "* `text_to_word_sequence`: Imports a function to split text into a list of words (useful for text pre-processing).\n",
    "* `matplotlib.pyplot`: For data visualization after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBTWYJe_vLzS"
   },
   "source": [
    "## **2. Load and Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ifoJngRQuC7Y"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Get word index dictionary from tokenizer used during data loading\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Convert data sequences to tensors (readable by the model)\n",
    "train_data = pad_sequences(train_data, maxlen=100)\n",
    "test_data = pad_sequences(test_data, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f6_Ns_8vR2C"
   },
   "source": [
    "This code performs the following actions:\n",
    "\n",
    "* Loads a subset of the IMDB movie review dataset with 10,000 most frequent words.\n",
    "* Obtain a reverse lookup for indices of all 10,000 words.\n",
    "* Splits the data into training and testing sets.\n",
    "* Converts the sequence of word indexes into padded tensors. Padding ensures all sequences have the same length (100 words in this case) for the model to process them effectively.\n",
    "\n",
    "\n",
    "### **Explanation:**\n",
    "\n",
    "   - The `imdb.load_data()` function loads the IMDB movie review dataset with only the top 10,000 most frequently occurring words (`num_words=10000`).\n",
    "   - The dataset is divided into training and testing sets, with data as sequences of word indices and labels as sentiment classifications (positive or negative).\n",
    "   - `imdb.get_word_index()` retrieves the dictionary that maps words to their corresponding integer indices. This dictionary was created when the IMDB dataset was initially preprocessed.\n",
    "   - `pad_sequences()` transforms the lists of word indices (which might be of  different lengths) into tensors of a fixed length (`maxlen=100`).\n",
    "   - This ensures all input sequences have the same length,  which is necessary for most neural network models.  Sequences shorter than 100 are padded with zeros at the end, and longer ones are truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS7qVYuDvjoH"
   },
   "source": [
    "## **3. Build the RNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24BEyglvuKdT",
    "outputId": "45d0c8c5-ccb4-4397-a01c-d82245f482d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                12352     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1292417 (4.93 MB)\n",
      "Trainable params: 1292417 (4.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "  keras.layers.SimpleRNN(units=64),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A4VKopdvpIh"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "Here, we build the RNN model step-by-step:\n",
    "\n",
    "* **Embedding layer:** This layer transforms each integer representing a word in the sequence into a dense vector (128 dimensions in this case). This captures semantic relationships between words.\n",
    "* **SimpleRNN layer:** This is the core recurrent layer with 64 hidden units. It processes the sequence of word vectors one by one, considering the information from previous steps.\n",
    "* **Dense layer:** This final layer takes the output from the RNN and compresses it into a single unit with a sigmoid activation function. Since we're predicting positive or negative sentiment (1 or 0), sigmoid is a good choice.\n",
    "\n",
    "We then compile the model by specifying:\n",
    "\n",
    "* **Loss function:** 'binary_crossentropy' is suitable for binary classification tasks like sentiment analysis.\n",
    "* **Optimizer:** 'adam' is a popular optimizer that efficiently adjusts model weights during training.\n",
    "* **Metrics:** We track 'accuracy' to measure how well the model predicts sentiment correctly.\n",
    "\n",
    "Finally, `model.summary()` provides a detailed overview of the model architecture, including the number of parameters and layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Eu8Wr1dwlgr"
   },
   "source": [
    "## **4. Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QKk71TMuN6d",
    "outputId": "b170b278-8e93-4d71-af00-7837dd2b9c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 92s 114ms/step - loss: 0.5376 - accuracy: 0.7175 - val_loss: 0.5636 - val_accuracy: 0.7346\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.3882 - accuracy: 0.8288 - val_loss: 0.4450 - val_accuracy: 0.8149\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.1938 - accuracy: 0.9249 - val_loss: 0.5299 - val_accuracy: 0.7960\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.0755 - accuracy: 0.9747 - val_loss: 0.6690 - val_accuracy: 0.7894\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 77s 99ms/step - loss: 0.0417 - accuracy: 0.9860 - val_loss: 0.8543 - val_accuracy: 0.7737\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vHmWqxZwsnR"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "* `model.fit `takes the training data, labels, the number of training epochs (iterations), and validation data for monitoring performance during training.\n",
    "* During training, the model learns to map sequences of words (reviews) to sentiment labels (positive or negative).\n",
    "\n",
    "* We've introduced the `batch_size` parameter to the `model.fit` function. During training, a deep learning model uses batches of data—subgroups of the entire training set—for efficient learning. A batch size of 32 is a common choice. This means that in each iteration (within an epoch), 32 training samples will be used to update the model's weights.\n",
    "\n",
    "### **How training works under the hood:**\n",
    "\n",
    "1. The training data is divided into batches.\n",
    "2. The model processes a single batch of data and predicts the labels for that batch.\n",
    "3. The model's loss (how far off the predictions are) is calculated.\n",
    "4. Gradients are computed to determine how to adjust the model's weights to reduce the loss.\n",
    "5. The optimizer uses the gradients to update the model's weights.\n",
    "6. Steps 2-5 are repeated for all batches in one epoch, and then again for multiple epochs.\n",
    "\n",
    "### **Key points**\n",
    "\n",
    "* **Validation data:** We use the validation data to see how well the model generalizes to unseen data. This helps avoid overfitting (where the model performs very well on the training data but poorly on held-out data).\n",
    "* **Experiment with:** Feel free to experiment with different `batch_size` values and numbers of `epochs` to see how they affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeKzWaJ8yGAB"
   },
   "source": [
    "## **5. Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qaEWfStubjU",
    "outputId": "d055aab8-54cc-45c4-dd4e-af9398bda5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8543 - accuracy: 0.7737\n",
      "Test Loss: 0.8543428778648376\n",
      "Test Accuracy: 0.7736799716949463\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Performance on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG3E1bESyLmh"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "`model.evaluate `calculates the loss and accuracy on the held-out testing set. This gives us a more unbiased assessment of how well the model has truly learned to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g316AyPBySYx"
   },
   "source": [
    "## **6. Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ml-PVXfyugmu",
    "outputId": "3a70f155-81f8-499c-f4e0-d188e710bd92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n",
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on new data\n",
    "new_review = \"This movie was fantastic! Great acting and an engaging plot.\"  # Example\n",
    "new_review = text_to_word_sequence(new_review)\n",
    "encoded_review = [word_index[word] for word in new_review if word in word_index]\n",
    "encoded_review = pad_sequences([encoded_review], maxlen=100)\n",
    "prediction = model.predict(encoded_review)\n",
    "\n",
    "# Interpret the prediction\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdjXgu-GAq4i"
   },
   "source": [
    "**Predicting Sentiment on a New Review**\n",
    "\n",
    "This code takes a new movie review as input and uses the trained RNN model to predict its sentiment (positive or negative). Here's a step-by-step breakdown:\n",
    "\n",
    "1. **Preprocess the Review:**\n",
    "   - **`new_review = text_to_word_sequence(new_review)`:** Splits the review into a list of individual words.\n",
    "   - **`encoded_review = [word_index[word] for word in new_review if word in word_index]`:** Converts the words to their corresponding integer indices from the `word_index` dictionary (filtering out words not in the vocabulary).\n",
    "   - **`encoded_review = pad_sequences([encoded_review], maxlen=100)`:** Pads or truncates the encoded review to a length of 100 to match the model's input shape.\n",
    "\n",
    "2. **Generate Prediction:**\n",
    "   - **`prediction = model.predict(encoded_review)`:** Feeds the preprocessed review to the trained RNN model, generating a probability between 0 and 1 (higher probability indicates positive sentiment).\n",
    "\n",
    "3. **Interpret the Prediction:**\n",
    "   - **`if prediction[0][0] > 0.5: ... else: ...`:** Checks the probability to determine the sentiment. If it's greater than 0.5, the review is considered positive; otherwise, it's considered negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVS-zmlPBdv8"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "This code demonstrates how you can use a trained RNN model to make predictions on unseen textual data. Remember that the quality of your predictions will depend on the size and representativeness of your training dataset, as well as the model's architecture.\n",
    "\n",
    "## **Extending Your Knowledge**\n",
    "\n",
    "* Explore more complex RNN architectures like LSTM and GRU cells: https://keras.io/api/layers/recurrent_layers/\n",
    "* Learn about pre-trained word embeddings to improve your model: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "* Experiment with different hyperparameters and regularization techniques: https://www.tensorflow.org/tutorials/keras/overfit_and_underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T4F9etzs7Jbo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
