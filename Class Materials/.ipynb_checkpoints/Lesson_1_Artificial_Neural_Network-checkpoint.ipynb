{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ32We8hk69b"
   },
   "source": [
    "# **Hands-On Exercise: Building Your First Artificial Neural Network**\n",
    "\n",
    "Understand the core concepts of building an artificial neural network. Get comfortable with the architecture of neurons and layers. Experience the process of training and evaluating a model.\n",
    "\n",
    "## **Prerequisites**\n",
    "Basic Python programming concepts (variables, functions, loops). A general understanding of what machine learning is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ehYQWS6KyQPE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# **Activation Function**\n",
    "def sigmoid(x):\n",
    "    \"\"\"Calculates the sigmoid activation for input values.\n",
    "    The sigmoid function 'squashes' values between 0 and 1.\n",
    "    This helps our neural network learn complex, non-linear patterns.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Python:\n",
    "- Triple-quoted strings (''' ... ''' or \"\"\" ... \"\"\") placed immediately after a function, method, class, or module definition are interpreted as __docstrings__.\n",
    "- A docstring is a special kind of string used to document what the function/class/module does.\n",
    "- Python doesn’t treat it as a code statement to be executed—rather, it’s stored as .__doc__ attribute of the function and is used by tools like help()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the Sigmoid activation function, let's move on to defining our next function, `train()` which will define and trains a very simple feedforward neural network with one hidden layer using sigmoid activation and manual weight updates via backpropagation.\n",
    "![](Lesson1SimpleNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Training the Neural Network**\n",
    "def train(X, y, hidden_neurons, output_neurons, epochs, learning_rate):\n",
    "    \"\"\"Trains a simple neural network.\n",
    "\n",
    "    Args:\n",
    "        X: Input data (features).\n",
    "        y: True labels/outputs associated with the input data (target).\n",
    "        hidden_neurons: Number of neurons (nodes) in the hidden layer.\n",
    "        output_neurons: Number of neurons (nodes) in the output layer.\n",
    "        epochs: Number of times to train over full dataset.\n",
    "        learning_rate: Controls how much we update weights during each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # **Initialize weights with random values**\n",
    "    input_weights = np.random.randn(X.shape[1], hidden_neurons) # generate weight matrix for connections between input & hidden layer\n",
    "    hidden_weights = np.random.randn(hidden_neurons, output_neurons) # generate weight matrix for connections between hidden & output layer\n",
    "    \n",
    "    # **Training Loop over given number of Epochs**\n",
    "    for _ in range(epochs):\n",
    "        # **Forward Propagation**\n",
    "        # Calculate input to the hidden layer\n",
    "        hidden_layer_input = np.dot(X, input_weights) # dot product/matrix multiplication between input data & input weights \n",
    "        # Apply the sigmoid activation to the hidden layer input\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input) \n",
    "\n",
    "        # Calculate hidden to the output layer\n",
    "        output_layer_input = np.dot(hidden_layer_output, hidden_weights) \n",
    "        # Apply the sigmoid activation to get the final predicted output\n",
    "        output = sigmoid(output_layer_input)\n",
    "\n",
    "        # **Backpropagation**\n",
    "        # Calculate how much our prediction was off\n",
    "        output_error = y - output\n",
    "        # Calculate the output layer gradient (how much to change output weights)\n",
    "        output_delta = output_error * output * (1 - output)\n",
    "\n",
    "        # Calculate the hidden layer error (how wrong were hidden layer outputs)\n",
    "        hidden_error = np.dot(output_delta, hidden_weights.T)\n",
    "        # Calculate the hidden layer gradient (how much to change hidden weights)\n",
    "        hidden_delta = hidden_error * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "        # **Update weights (adjust weights to reduce error)**\n",
    "        hidden_weights += learning_rate * np.dot(hidden_layer_output.T, output_delta) # dot product to update weights for all nodes at once\n",
    "        input_weights += learning_rate * np.dot(X.T, hidden_delta)\n",
    "\n",
    "    return input_weights, hidden_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some footnotes on the code above:\n",
    "- `X.shape[1]` - If X is a Pandas DataFrame (or a NumPy 2D array), then: X.shape returns a tuple of (number of rows, number of columns), so X.shape[1] will return number of columns = number of input nodes in the first layer (input layer of our NN model).\n",
    "- `np.random.randn(X.shape[1], hidden_neurons)` - Generates a matrix of shape (a, b) filled with random values, where a = number of feature columns in input data and b = hidden layer nodes passed as an argument to the function.\n",
    "- Weights update:\n",
    "    - `output_error = y - output` - Measures how wrong the prediction is. This is the derivative of the loss function.\n",
    "    - `output * (1 - output)` - First order derivative of the sigmoid function. σ′(x)=σ(x)⋅(1−σ(x))\n",
    "    - `output_delta = output_error * output * (1 - output)` - Chain rule is being applied to calculate the _Loss w.r.t. weights_ which tells how much to change output layer weights.\n",
    "        - __Loss w.r.t. weights = Loss w.r.t. output × output w.r.t. activation input__\n",
    "- `hidden_error` - We can't directly compare the hidden layer's output with the true label — because the hidden layer doesn't produce predictions. So instead, we pass the output layer's error backward into the hidden layer — using the output weights.\n",
    "    - We do this by: Taking the output_delta (how much each output neuron needs to change). And projecting it backwards through the weights between hidden and output layers. The .T (transpose) is used to align the matrix shapes properly.\n",
    "- `hidden_weights` - This is essentially the implementation of new_weights = old_weights - learning_rate * gradient. We're using weights += learning_rate * negative_gradient due to our negative gradient computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL15i5Kly8-7",
    "outputId": "1d327909-3f29-4662-ea72-8e367c437cf4"
   },
   "outputs": [],
   "source": [
    "# **Using the Trained Network**\n",
    "def predict(X, input_weights, hidden_weights):\n",
    "    \"\"\"Makes predictions using the trained neural network.\"\"\"\n",
    "    hidden_layer_input = np.dot(X, input_weights) # uses input_weights from train() function earlier\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, hidden_weights) # uses hidden_weights from train() function earlier\n",
    "    output = sigmoid(output_layer_input)\n",
    "    return output # return final prediction of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39188572]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with a very simple dataset (XOR boolean logic)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])  # Simple XOR function\n",
    "\n",
    "# Train network with 2 hidden layer nodes, 1 output layer node for 0/1 prediction\n",
    "# Train over 10k full cycles and 0.1 step size for weights adjustment\n",
    "input_weights, hidden_weights = train(X, y, \n",
    "                                      hidden_neurons=2, \n",
    "                                      output_neurons=1, \n",
    "                                      epochs=10000, \n",
    "                                      learning_rate=0.1)\n",
    "\n",
    "new_input = np.array([[1, 1]])\n",
    "print(predict(new_input, input_weights, hidden_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays for `X` and `y` are defining this.\n",
    "Sample XOR problem - __Exclusive OR__ Logical Operation, where the Output is 1 only if exactly one input is 1 (i.e., the inputs are different).\n",
    "| Input A | Input B | Output: A XOR B |\n",
    "|---------|---------|---------|\n",
    "|    0    |    0    |    0    |\n",
    "|    0    |    1    |    1    |\n",
    "|    1    |    0    |    1    |\n",
    "|    1    |    1    |    0    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbDfKYohlv0Y"
   },
   "source": [
    "## Demystifying the Neural Network: A Breakdown\n",
    "\n",
    "Imagine a simple brain with interconnected processing units (neurons) mimicking how we learn. This code builds a basic version of that. Let's break it down:\n",
    "\n",
    "* **Input Layer:** This is where we feed data (like numbers) into the network. For the XOR example, this layer takes two numbers (0 or 1) as input.\n",
    "* **Hidden Layer(s):**  These are the heart of the network, containing multiple interconnected neurons. They perform calculations and transformations on the data.\n",
    "* **Output Layer:** Here, the final prediction or answer from the network emerges. In the XOR case, the output layer has one neuron, predicting a 0 or a 1.\n",
    "\n",
    "**Activation Function:**  This function acts like a gatekeeper, controlling how strongly a neuron \"fires\" based on its calculations. It introduces non-linearity, allowing the network to learn complex patterns.\n",
    "   * **Sigmoid Function:** A common activation function, it 'squashes' input values into a range between 0 and 1. This helps in tasks like predicting probabilities (in this case, the probability of the output being 0 or 1).\n",
    "\n",
    "**Learning Process:**\n",
    "\n",
    "1. **Forward Propagation:**  Information flows from input, through hidden layers (with calculations at each step), and reaches the output.\n",
    "2. **Backward Propagation (Backprop):**  We compare the output with the correct answer (like a teacher grading a test). Backprop helps the network adjust the connections between neurons (like rewiring) to minimize errors in future predictions.\n",
    "\n",
    "**Cost Function:**  (Not explicitly coded here, but plays a role in backpropagation). This function measures how wrong the network's prediction was. Backprop uses this to determine how much to adjust the connections.\n",
    "\n",
    "**What is the network predicting?**\n",
    "\n",
    "The network is essentially learning a rule for the XOR function. XOR outputs a 1 only when the two inputs are different (0, 1) or (1, 0). In any other case (both 0s or both 1s), the XOR function outputs a 0. The goal of the network is to train the hidden layer neurons to process the input values and activate the output neuron accordingly, mimicking this XOR logic.\n",
    "\n",
    "**Through many training iterations (epochs) with adjustments based on backprop, the network gradually learns to make better predictions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqE3X1KZnCjv"
   },
   "source": [
    "## A Better Model\n",
    "The above neural network did not do a very good job at '*learning*' and then '*predicting*' the correct output of 0, when given an input of (1, 1).\n",
    "\n",
    "The next model below will do a better job at that. It includes potential changes to improve prediction performance over the first (XOR-based) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cCuNDKuoy9xc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# A revised training function\n",
    "def train(X, y, hidden_neurons, output_neurons, epochs, learning_rate):\n",
    "    input_weights = np.random.randn(X.shape[1], hidden_neurons)\n",
    "    hidden_weights = np.random.randn(hidden_neurons, output_neurons)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward Propagation (no changes here)\n",
    "        hidden_layer_input = np.dot(X, input_weights)\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, hidden_weights)\n",
    "        output = sigmoid(output_layer_input)\n",
    "\n",
    "        # Backpropagation\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * output * (1 - output)\n",
    "\n",
    "        hidden_error = np.dot(output_delta, hidden_weights.T)\n",
    "        hidden_delta = hidden_error * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "        # Update weights with a smaller learning rate\n",
    "        hidden_weights += learning_rate * np.dot(hidden_layer_output.T, output_delta)\n",
    "        input_weights += learning_rate * np.dot(X.T, hidden_delta)\n",
    "\n",
    "    return input_weights, hidden_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WU-pbZW8RfF-",
    "outputId": "be715b33-72d5-4db9-f8b7-7b5c5fc1c00a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04226242]]\n"
     ]
    }
   ],
   "source": [
    "# ... (predict function remains the same) ...\n",
    "\n",
    "# Example usage with adjusted parameters\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_weights, hidden_weights = train(X, y, \n",
    "                                      hidden_neurons=3, \n",
    "                                      output_neurons=1, \n",
    "                                      epochs=20000, \n",
    "                                      learning_rate=0.05)\n",
    "\n",
    "new_input = np.array([[1, 1]])\n",
    "print(predict(new_input, input_weights, hidden_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOmpr6_AoEeb"
   },
   "source": [
    "**Changes Made:**\n",
    "\n",
    "* **More Hidden Neurons:** Increased to 3 for slightly more modeling capacity.\n",
    "* **Increased Epochs:** 20000 epochs provide more training time.\n",
    "* **Decreased Learning Rate:** A smaller learning rate (0.05) takes smaller steps to avoid overshooting good solutions.\n",
    "\n",
    "**Caveats**\n",
    "\n",
    "* **XOR Problem:** Even with these changes, the basic XOR dataset may not be the best for showcasing the network's power.\n",
    "* **Hyperparameter Experimentation:** The best settings for neurons, epochs, and learning rate are found through experimentation.\n",
    "\n",
    "**Further Improvement Ideas**\n",
    "\n",
    "* **Add a Second Hidden Layer:** Makes the network more complex to handle nonlinear patterns.\n",
    "* **Early Stopping:** Implement a basic early stopping check to prevent overfitting if performance on a held-out validation set worsens."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
