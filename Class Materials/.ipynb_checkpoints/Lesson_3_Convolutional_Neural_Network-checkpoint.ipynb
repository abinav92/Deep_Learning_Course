{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI6Dz89XniMJ"
   },
   "source": [
    "# **Convolutional Neural Network for Fashion MNIST Classification with TensorFlow**\n",
    "This notebook provides a hands-on exercise to build a Convolutional Neural Network (CNN) model for classifying fashion items from the Fashion-MNIST dataset using TensorFlow.\n",
    "\n",
    "## **Learning Objectives:**\n",
    "\n",
    "* Understand the steps involved in building and training a CNN model.\n",
    "* Apply convolutional layers and pooling layers for image feature extraction.\n",
    "* Train the model on the Fashion-MNIST dataset.\n",
    "* Evaluate the model's performance.\n",
    "* Make predictions on new images.\n",
    "* Visualize learned features.\n",
    "\n",
    "## **Prerequisites:**\n",
    "\n",
    "* Basic understanding of Python programming\n",
    "* Familiarity with Machine Learning and Deep Learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCnC4s4Ihsci"
   },
   "source": [
    "## **The Fashion MNIST dataset**\n",
    "\n",
    "We wil be using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7SivmJEnx3A"
   },
   "source": [
    "## **1: Import Libraries and Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3YFN7TDl5Jg",
    "outputId": "0cc95c30-0d9d-44f1-dd98-5bac04519193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n",
      "Train images shape: (60000, 28, 28, 1)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28, 1)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the Fashion-MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (optional but recommended)\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape images for CNN input (add a channel dimension)\n",
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Print the data shapes\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idym8nZsn2NA"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1. We import necessary libraries: TensorFlow for deep learning operations, `fashion_mnist` for loading the dataset, and Keras for building the CNN model.\n",
    "2. We load the Fashion-MNIST dataset using `fashion_mnist.load_data()`. This function returns four NumPy arrays: training images, training labels, testing images, and testing labels.\n",
    "3. Pixel values in the images range from 0 to 255. It's a common practice to normalize them between 0 and 1 for better training performance.\n",
    "4. We reshape the images to add a channel dimension (1 for grayscale images). CNNs typically expect data with channels (e.g., RGB for color images).\n",
    "    -  CNN expects input in this format: (number of images, image height, image width, number of channels)\n",
    "    -  reshape(`-1`,...): This is a shortcut that tells Python, \"Figure this out for me based on the total number of images.\"\n",
    "    -  reshape(..,`28, 28,`...): height and width of each image — the Fashion-MNIST dataset has 28×28 pixel grayscale images.\n",
    "    -  reshape(...,`1`): number of color channels - 1 = grayscale (black & white) and 3 = RGB color image. Note that our dataset is grayscale images.\n",
    "6. We print the shapes of the training and testing data to understand their dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFAOlmsxiUQa"
   },
   "source": [
    "Loading the dataset returns four NumPy arrays:\n",
    "\n",
    "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
    "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays.\n",
    "\n",
    "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In the dataset, each image is mapped to a single label, which is an integer number, not a *class name*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6eAZOUzoDpY"
   },
   "source": [
    "## **2: Define the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhhmgQ53l6HI",
    "outputId": "b4295585-7c09-4dc3-8a7a-ed6584bae2c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121930 (476.29 KB)\n",
      "Trainable params: 121930 (476.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "  # Convolutional layer 1\n",
    "  Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  # Max pooling layer 1\n",
    "  MaxPooling2D((2, 2)),\n",
    "\n",
    "  # Convolutional layer 2\n",
    "  Conv2D(64, (3, 3), activation='relu'),\n",
    "  # Max pooling layer 2\n",
    "  MaxPooling2D((2, 2)),\n",
    "\n",
    "  # Flatten layer to convert 2D feature maps to 1D vector\n",
    "  Flatten(),\n",
    "\n",
    "  # Dense layer 1 with 64 units and ReLU activation\n",
    "  Dense(64, activation='relu'),\n",
    "\n",
    "  # Output layer with 10 units (one for each clothing category) \n",
    "  # and softmax activation for probability distribution\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjaJ9NveoPyA"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "* **Convolutional Layers:**\n",
    "\n",
    "    * The two `Conv2D` layers are the core feature extractors of our CNN. They use small filters (3x3) to slide over the input image and identify patterns.\n",
    "    * We start with 32 filters in the first layer and increase to 64 filters in the second. The idea is to progressively extract more complex features as we go deeper into the network.\n",
    "    * The `relu` activation function introduces non-linearity. It helps prevent the model from learning simple linear relationships.\n",
    "* **Max Pooling Layers:**\n",
    "\n",
    "    * `MaxPooling2D` layers reduce the dimensionality of the feature maps. This:\n",
    "        * Makes computation faster\n",
    "        * Helps the model focus on the most important features\n",
    "        * Provides some translational invariance (i.e., the ability to recognize features even if they're slightly shifted in the image).\n",
    "* **Flatten Layer:**\n",
    "\n",
    "    * The `Flatten` layer prepares the output of the convolutional layers for the dense layers. It converts the 2D feature maps into one long vector.\n",
    "* **Dense Layers:**\n",
    "\n",
    "    * The dense layers act as traditional neural network layers. They interpret the extracted features from the convolutional part of the model.\n",
    "    * The last `Dense` layer has ten units, one for each fashion class. The `softmax` activation calculates probabilities for each of the classes.\n",
    "* **Compilation:**\n",
    "\n",
    "    * `optimizer`: We choose 'adam', a common optimizer that adaptively updates learning rates.\n",
    "    * `loss`: 'sparse_categorical_crossentropy' is used for multi-class classification problems where the labels are integers. We're using sparse_categorical_crossentropy because our labels have NOT been one-hot encoded.\n",
    "    * `metrics`: We track 'accuracy' to evaluate how often the model correctly predicts the clothing category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the model compilation summary:**\n",
    "1. `conv2d (Conv2D)`:\n",
    "- Input shape was (28, 28, 1) — grayscale image. Output shape: (None, 26, 26, 32)\n",
    "    - 32 filters (kernels) → gives 32 output channels.\n",
    "    - 3×3 filter with default stride of 1 reduces width/height by 2 (no padding).\n",
    "    - Param #: 320\n",
    "    - Formula: (filter_height × filter_width × input_channels + 1) × filters. (3×3×1 + 1) × 32 = 320\n",
    "2. `max_pooling2d`:\n",
    "- Output shape: (None, 13, 13, 32)\n",
    "    - 2×2 pooling cuts each spatial dimension in half.\n",
    "    - Param #: 0 (Pooling has no weights — it just picks max values.)\n",
    "\n",
    "And so on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-otAqUMpqkp"
   },
   "source": [
    "## **3: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MM1G59T9mULz",
    "outputId": "4cd3c45f-0e6f-4ea1-a3f9-f5ff76065677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 5ms/step - loss: 0.4569 - accuracy: 0.8360 - val_loss: 0.3504 - val_accuracy: 0.8739\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3067 - accuracy: 0.8885 - val_loss: 0.3290 - val_accuracy: 0.8761\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2624 - accuracy: 0.9038 - val_loss: 0.2860 - val_accuracy: 0.8972\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2315 - accuracy: 0.9146 - val_loss: 0.2693 - val_accuracy: 0.9013\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2073 - accuracy: 0.9232 - val_loss: 0.2649 - val_accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, \n",
    "                    epochs=5, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hfepr0Ep1_m"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1. We use the `fit` method of the model to train the CNN.\n",
    "2. We pass the training images (`train_images`) and their corresponding labels (`train_labels`) as input.\n",
    "3. We train for 5 epochs (iterations over the entire dataset).\n",
    "4. We evaluate the model's performance on unseen data using the validation set (`test_images`, `test_labels`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFV6sW7SqDKg"
   },
   "source": [
    "## **4: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyRGqz2vmbLN",
    "outputId": "ce975fb0-24d1-4f3c-f612-61d910c4fa5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.2649 - accuracy: 0.9065 - 601ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.906499981880188\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsQaMivwqJBj"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1. We use the `evaluate` method to get the loss and accuracy of the model on the test set.\n",
    "2. `verbose=2` will output a progress bar during the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYJEGRidqPg5"
   },
   "source": [
    "## **5: Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kt4QrNakmeDd",
    "outputId": "4eab42ed-b027-455f-a64d-0ae2b16a031f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Predicted class for the first test image: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make predictions on new images\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Example of predicting the class for the first image in the test set\n",
    "print(\"Predicted class for the first test image:\", np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0zk9KV1qTgH"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1. The `predict` method generates predictions for new images (in this case, from the test set).\n",
    "2. `np.argmax(predictions[0])` extracts the class with the highest probability for the first image in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1DXCGy8qYyZ"
   },
   "source": [
    "## **6: Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "jJ9bLHDvmiaA",
    "outputId": "6af01738-09a0-402e-f486-7b4f85588547"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot an image along with its predicted class\n",
    "def plot_image_prediction(image, prediction, true_label, class_names):\n",
    "  plt.imshow(image.reshape(28, 28), cmap=plt.cm.binary) # Display the image\n",
    "  predicted_label = class_names[np.argmax(prediction)]\n",
    "  true_label = class_names[true_label]\n",
    "  plt.title(\"Predicted: {}, True: {}\".format(predicted_label, true_label))\n",
    "  plt.axis('off')\n",
    "\n",
    "# Class names for the dataset\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Plot first image from test set with its predicted label\n",
    "plot_image_prediction(test_images[0], predictions[0], test_labels[0], class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3uNLrQqfzB"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1. We import `matplotlib.pyplot` for plotting.\n",
    "2. The `plot_image_prediction` function helps visualize an image and its prediction.\n",
    "3. The class names are defined to have readable labels.\n",
    "4. We plot an example image with its prediction using the defined function.\n",
    "\n",
    "### **Important Notes**\n",
    "\n",
    "* This is a basic CNN example. You can achieve higher accuracy with more complex architectures, data augmentation, and hyperparameter tuning.\n",
    "* Experiment with the filter sizes, number of layers, and number of epochs to see how those affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twA5u4kiq8Lc"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "In this hands-on exercise, you built a basic Convolutional Neural Network to classify images from the Fashion-MNIST dataset. Here's what you've accomplished:\n",
    "\n",
    "* **Learned about CNN architectures:** You saw how convolutional layers, pooling layers, and dense layers work together to extract features and classify images.\n",
    "* **Trained a CNN model:** You fit the model to the training data, learning to distinguish between different clothing items.\n",
    "* **Evaluated your model:** You calculated the accuracy on the test set, getting a sense of its generalization capabilities.\n",
    "* **Visualized results:**  You visualized the model's predictions to understand its decision-making process.\n",
    "\n",
    "**Next Steps: Exploring More**\n",
    "\n",
    "* **Experiment and improve:** Try adjusting the model architecture, the number of epochs, and optimization techniques to see how they  improve accuracy.\n",
    "* **Data Augmentation:** Increase the size and variation of your training set using image transformations (rotations, flips, etc.) to make your model more robust.\n",
    "* **Advanced CNN Architectures:** Explore state-of-the-art architectures like ResNet, VGG, and DenseNet.\n",
    "\n",
    "**References for Further Learning**\n",
    "\n",
    "* **TensorFlow/Keras documentation:** [https://www.tensorflow.org/](https://www.tensorflow.org/)\n",
    "* **Deep Learning book (Ian Goodfellow, Yoshua Bengio, and Aaron Courville):** [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)\n",
    "* **Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (Aurélien Géron)**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
